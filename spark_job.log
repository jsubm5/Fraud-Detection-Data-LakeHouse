24/08/25 20:52:06 INFO SparkContext: Running Spark version 3.5.2
24/08/25 20:52:07 INFO SparkContext: OS info Linux, 6.8.0-40-generic, amd64
24/08/25 20:52:07 INFO SparkContext: Java version 11.0.24
24/08/25 20:52:07 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable
24/08/25 20:52:07 INFO ResourceUtils: ==============================================================
24/08/25 20:52:07 INFO ResourceUtils: No custom resources configured for spark.driver.
24/08/25 20:52:07 INFO ResourceUtils: ==============================================================
24/08/25 20:52:07 INFO SparkContext: Submitted application: KafkaTransactionConsumer
24/08/25 20:52:07 INFO ResourceProfile: Default ResourceProfile created, executor resources: Map(cores -> name: cores, amount: 1, script: , vendor: , memory -> name: memory, amount: 1024, script: , vendor: , offHeap -> name: offHeap, amount: 0, script: , vendor: ), task resources: Map(cpus -> name: cpus, amount: 1.0)
24/08/25 20:52:07 INFO ResourceProfile: Limiting resource is cpu
24/08/25 20:52:07 INFO ResourceProfileManager: Added ResourceProfile id: 0
24/08/25 20:52:08 INFO SecurityManager: Changing view acls to: root
24/08/25 20:52:08 INFO SecurityManager: Changing modify acls to: root
24/08/25 20:52:08 INFO SecurityManager: Changing view acls groups to: 
24/08/25 20:52:08 INFO SecurityManager: Changing modify acls groups to: 
24/08/25 20:52:08 INFO SecurityManager: SecurityManager: authentication disabled; ui acls disabled; users with view permissions: root; groups with view permissions: EMPTY; users with modify permissions: root; groups with modify permissions: EMPTY
24/08/25 20:52:09 INFO Utils: Successfully started service 'sparkDriver' on port 46569.
24/08/25 20:52:09 INFO SparkEnv: Registering MapOutputTracker
24/08/25 20:52:09 INFO SparkEnv: Registering BlockManagerMaster
24/08/25 20:52:09 INFO BlockManagerMasterEndpoint: Using org.apache.spark.storage.DefaultTopologyMapper for getting topology information
24/08/25 20:52:09 INFO BlockManagerMasterEndpoint: BlockManagerMasterEndpoint up
24/08/25 20:52:09 INFO SparkEnv: Registering BlockManagerMasterHeartbeat
24/08/25 20:52:09 INFO DiskBlockManager: Created local directory at /tmp/blockmgr-22d4486f-70be-4154-a450-9ce77f058fde
24/08/25 20:52:09 INFO MemoryStore: MemoryStore started with capacity 434.4 MiB
24/08/25 20:52:09 INFO SparkEnv: Registering OutputCommitCoordinator
24/08/25 20:52:10 INFO JettyUtils: Start Jetty 0.0.0.0:4040 for SparkUI
24/08/25 20:52:10 INFO Utils: Successfully started service 'SparkUI' on port 4040.
24/08/25 20:52:10 INFO Executor: Starting executor ID driver on host 8ef98fa209cc
24/08/25 20:52:10 INFO Executor: OS info Linux, 6.8.0-40-generic, amd64
24/08/25 20:52:10 INFO Executor: Java version 11.0.24
24/08/25 20:52:10 INFO Executor: Starting executor with user classpath (userClassPathFirst = false): ''
24/08/25 20:52:10 INFO Executor: Created or updated repl class loader org.apache.spark.util.MutableURLClassLoader@371bf8a9 for default.
24/08/25 20:52:10 INFO Utils: Successfully started service 'org.apache.spark.network.netty.NettyBlockTransferService' on port 34879.
24/08/25 20:52:10 INFO NettyBlockTransferService: Server created on 8ef98fa209cc:34879
24/08/25 20:52:10 INFO BlockManager: Using org.apache.spark.storage.RandomBlockReplicationPolicy for block replication policy
24/08/25 20:52:10 INFO BlockManagerMaster: Registering BlockManager BlockManagerId(driver, 8ef98fa209cc, 34879, None)
24/08/25 20:52:10 INFO BlockManagerMasterEndpoint: Registering block manager 8ef98fa209cc:34879 with 434.4 MiB RAM, BlockManagerId(driver, 8ef98fa209cc, 34879, None)
24/08/25 20:52:10 INFO BlockManagerMaster: Registered BlockManager BlockManagerId(driver, 8ef98fa209cc, 34879, None)
24/08/25 20:52:10 INFO BlockManager: Initialized BlockManager: BlockManagerId(driver, 8ef98fa209cc, 34879, None)
24/08/25 20:52:12 INFO SharedState: Setting hive.metastore.warehouse.dir ('null') to the value of spark.sql.warehouse.dir.
24/08/25 20:52:12 INFO SharedState: Warehouse path is 'file:/opt/spark/spark-warehouse'.
24/08/25 20:52:27 INFO CodeGenerator: Code generated in 875.914749 ms
24/08/25 20:52:28 INFO SparkContext: Starting job: json at NativeMethodAccessorImpl.java:0
24/08/25 20:52:28 INFO DAGScheduler: Got job 0 (json at NativeMethodAccessorImpl.java:0) with 8 output partitions
24/08/25 20:52:28 INFO DAGScheduler: Final stage: ResultStage 0 (json at NativeMethodAccessorImpl.java:0)
24/08/25 20:52:28 INFO DAGScheduler: Parents of final stage: List()
24/08/25 20:52:28 INFO DAGScheduler: Missing parents: List()
24/08/25 20:52:28 INFO DAGScheduler: Submitting ResultStage 0 (MapPartitionsRDD[6] at json at NativeMethodAccessorImpl.java:0), which has no missing parents
24/08/25 20:52:28 INFO MemoryStore: Block broadcast_0 stored as values in memory (estimated size 21.1 KiB, free 434.4 MiB)
24/08/25 20:52:28 INFO MemoryStore: Block broadcast_0_piece0 stored as bytes in memory (estimated size 10.3 KiB, free 434.4 MiB)
24/08/25 20:52:28 INFO BlockManagerInfo: Added broadcast_0_piece0 in memory on 8ef98fa209cc:34879 (size: 10.3 KiB, free: 434.4 MiB)
24/08/25 20:52:28 INFO SparkContext: Created broadcast 0 from broadcast at DAGScheduler.scala:1585
24/08/25 20:52:28 INFO DAGScheduler: Submitting 8 missing tasks from ResultStage 0 (MapPartitionsRDD[6] at json at NativeMethodAccessorImpl.java:0) (first 15 tasks are for partitions Vector(0, 1, 2, 3, 4, 5, 6, 7))
24/08/25 20:52:28 INFO TaskSchedulerImpl: Adding task set 0.0 with 8 tasks resource profile 0
24/08/25 20:52:29 INFO TaskSetManager: Starting task 0.0 in stage 0.0 (TID 0) (8ef98fa209cc, executor driver, partition 0, PROCESS_LOCAL, 8979 bytes) 
24/08/25 20:52:29 INFO TaskSetManager: Starting task 1.0 in stage 0.0 (TID 1) (8ef98fa209cc, executor driver, partition 1, PROCESS_LOCAL, 8979 bytes) 
24/08/25 20:52:29 INFO TaskSetManager: Starting task 2.0 in stage 0.0 (TID 2) (8ef98fa209cc, executor driver, partition 2, PROCESS_LOCAL, 8979 bytes) 
24/08/25 20:52:29 INFO TaskSetManager: Starting task 3.0 in stage 0.0 (TID 3) (8ef98fa209cc, executor driver, partition 3, PROCESS_LOCAL, 8979 bytes) 
24/08/25 20:52:29 INFO TaskSetManager: Starting task 4.0 in stage 0.0 (TID 4) (8ef98fa209cc, executor driver, partition 4, PROCESS_LOCAL, 8979 bytes) 
24/08/25 20:52:29 INFO TaskSetManager: Starting task 5.0 in stage 0.0 (TID 5) (8ef98fa209cc, executor driver, partition 5, PROCESS_LOCAL, 8979 bytes) 
24/08/25 20:52:29 INFO TaskSetManager: Starting task 6.0 in stage 0.0 (TID 6) (8ef98fa209cc, executor driver, partition 6, PROCESS_LOCAL, 8979 bytes) 
24/08/25 20:52:29 INFO TaskSetManager: Starting task 7.0 in stage 0.0 (TID 7) (8ef98fa209cc, executor driver, partition 7, PROCESS_LOCAL, 9452 bytes) 
24/08/25 20:52:29 INFO Executor: Running task 7.0 in stage 0.0 (TID 7)
24/08/25 20:52:29 INFO Executor: Running task 3.0 in stage 0.0 (TID 3)
24/08/25 20:52:29 INFO Executor: Running task 5.0 in stage 0.0 (TID 5)
24/08/25 20:52:29 INFO Executor: Running task 2.0 in stage 0.0 (TID 2)
24/08/25 20:52:29 INFO Executor: Running task 4.0 in stage 0.0 (TID 4)
24/08/25 20:52:29 INFO Executor: Running task 6.0 in stage 0.0 (TID 6)
24/08/25 20:52:29 INFO Executor: Running task 0.0 in stage 0.0 (TID 0)
24/08/25 20:52:29 INFO Executor: Running task 1.0 in stage 0.0 (TID 1)
24/08/25 20:52:32 INFO CodeGenerator: Code generated in 288.890982 ms
24/08/25 20:52:33 INFO PythonRunner: Times: total = 3283, boot = 2344, init = 939, finish = 0
24/08/25 20:52:33 INFO PythonRunner: Times: total = 3279, boot = 2334, init = 944, finish = 1
24/08/25 20:52:33 INFO PythonRunner: Times: total = 3335, boot = 2366, init = 969, finish = 0
24/08/25 20:52:33 INFO PythonRunner: Times: total = 3353, boot = 2433, init = 920, finish = 0
24/08/25 20:52:33 INFO PythonRunner: Times: total = 3405, boot = 2473, init = 932, finish = 0
24/08/25 20:52:33 INFO Executor: Finished task 2.0 in stage 0.0 (TID 2). 1936 bytes result sent to driver
24/08/25 20:52:33 INFO Executor: Finished task 6.0 in stage 0.0 (TID 6). 1936 bytes result sent to driver
24/08/25 20:52:33 INFO Executor: Finished task 3.0 in stage 0.0 (TID 3). 1936 bytes result sent to driver
24/08/25 20:52:33 INFO PythonRunner: Times: total = 3481, boot = 2378, init = 1103, finish = 0
24/08/25 20:52:33 INFO Executor: Finished task 4.0 in stage 0.0 (TID 4). 1936 bytes result sent to driver
24/08/25 20:52:33 INFO Executor: Finished task 0.0 in stage 0.0 (TID 0). 1936 bytes result sent to driver
24/08/25 20:52:33 INFO Executor: Finished task 5.0 in stage 0.0 (TID 5). 1936 bytes result sent to driver
24/08/25 20:52:33 INFO PythonRunner: Times: total = 3557, boot = 2356, init = 1200, finish = 1
24/08/25 20:52:33 INFO TaskSetManager: Finished task 3.0 in stage 0.0 (TID 3) in 4281 ms on 8ef98fa209cc (executor driver) (1/8)
24/08/25 20:52:33 INFO Executor: Finished task 1.0 in stage 0.0 (TID 1). 1936 bytes result sent to driver
24/08/25 20:52:33 INFO TaskSetManager: Finished task 4.0 in stage 0.0 (TID 4) in 4304 ms on 8ef98fa209cc (executor driver) (2/8)
24/08/25 20:52:33 INFO TaskSetManager: Finished task 0.0 in stage 0.0 (TID 0) in 4371 ms on 8ef98fa209cc (executor driver) (3/8)
24/08/25 20:52:33 INFO TaskSetManager: Finished task 5.0 in stage 0.0 (TID 5) in 4304 ms on 8ef98fa209cc (executor driver) (4/8)
24/08/25 20:52:33 INFO TaskSetManager: Finished task 2.0 in stage 0.0 (TID 2) in 4317 ms on 8ef98fa209cc (executor driver) (5/8)
24/08/25 20:52:33 INFO PythonRunner: Times: total = 3519, boot = 2398, init = 1120, finish = 1
24/08/25 20:52:33 INFO TaskSetManager: Finished task 6.0 in stage 0.0 (TID 6) in 4317 ms on 8ef98fa209cc (executor driver) (6/8)
24/08/25 20:52:33 INFO TaskSetManager: Finished task 1.0 in stage 0.0 (TID 1) in 4335 ms on 8ef98fa209cc (executor driver) (7/8)
24/08/25 20:52:33 INFO Executor: Finished task 7.0 in stage 0.0 (TID 7). 2716 bytes result sent to driver
24/08/25 20:52:33 INFO PythonAccumulatorV2: Connected to AccumulatorServer at host: 127.0.0.1 port: 52921
24/08/25 20:52:33 INFO TaskSetManager: Finished task 7.0 in stage 0.0 (TID 7) in 4349 ms on 8ef98fa209cc (executor driver) (8/8)
24/08/25 20:52:33 INFO TaskSchedulerImpl: Removed TaskSet 0.0, whose tasks have all completed, from pool 
24/08/25 20:52:33 INFO DAGScheduler: ResultStage 0 (json at NativeMethodAccessorImpl.java:0) finished in 5.187 s
24/08/25 20:52:33 INFO DAGScheduler: Job 0 is finished. Cancelling potential speculative or zombie tasks for this job
24/08/25 20:52:33 INFO TaskSchedulerImpl: Killing all running tasks in stage 0: Stage finished
24/08/25 20:52:33 INFO DAGScheduler: Job 0 finished: json at NativeMethodAccessorImpl.java:0, took 5.362352 s
INFO:py4j.java_gateway:Callback Server Starting
INFO:py4j.java_gateway:Socket listening on ('127.0.0.1', 34299)
24/08/25 20:52:34 INFO StateStoreCoordinatorRef: Registered StateStoreCoordinator endpoint
24/08/25 20:52:34 WARN ResolveWriteToStream: Temporary checkpoint location created which is deleted normally when the query didn't fail: /tmp/temporary-d5544bc3-2d70-4e62-a908-6946c745f073. If it's required to delete it under any circumstances, please set spark.sql.streaming.forceDeleteTempCheckpointLocation to true. Important to know deleting temp checkpoint folder is best effort.
24/08/25 20:52:34 INFO ResolveWriteToStream: Checkpoint root file:///tmp/temporary-d5544bc3-2d70-4e62-a908-6946c745f073 resolved to file:/tmp/temporary-d5544bc3-2d70-4e62-a908-6946c745f073.
24/08/25 20:52:34 WARN ResolveWriteToStream: spark.sql.adaptive.enabled is not supported in streaming DataFrames/Datasets and will be disabled.
24/08/25 20:52:35 INFO CheckpointFileManager: Writing atomically to file:/tmp/temporary-d5544bc3-2d70-4e62-a908-6946c745f073/metadata using temp file file:/tmp/temporary-d5544bc3-2d70-4e62-a908-6946c745f073/.metadata.54bceb30-05a2-420c-b76c-1b2ac3143c47.tmp
24/08/25 20:52:35 INFO CheckpointFileManager: Renamed temp file file:/tmp/temporary-d5544bc3-2d70-4e62-a908-6946c745f073/.metadata.54bceb30-05a2-420c-b76c-1b2ac3143c47.tmp to file:/tmp/temporary-d5544bc3-2d70-4e62-a908-6946c745f073/metadata
24/08/25 20:52:35 INFO MicroBatchExecution: Starting [id = 92810e3c-5257-45d9-9e8a-10b4006ae532, runId = 40ff6c75-883f-4214-b09a-045119f11a66]. Use file:/tmp/temporary-d5544bc3-2d70-4e62-a908-6946c745f073 to store the query checkpoint.
24/08/25 20:52:35 INFO MicroBatchExecution: Reading table [org.apache.spark.sql.kafka010.KafkaSourceProvider$KafkaTable@5ef398cb] from DataSourceV2 named 'kafka' [org.apache.spark.sql.kafka010.KafkaSourceProvider@1b142470]
24/08/25 20:52:35 INFO OffsetSeqLog: BatchIds found from listing: 
24/08/25 20:52:35 INFO OffsetSeqLog: BatchIds found from listing: 
24/08/25 20:52:35 INFO MicroBatchExecution: Starting new streaming query.
24/08/25 20:52:35 INFO MicroBatchExecution: Stream started from {}
24/08/25 20:52:35 INFO BlockManagerInfo: Removed broadcast_0_piece0 on 8ef98fa209cc:34879 in memory (size: 10.3 KiB, free: 434.4 MiB)
24/08/25 20:52:36 INFO AdminClientConfig: AdminClientConfig values: 
	bootstrap.servers = [kafka:9092]
	client.dns.lookup = use_all_dns_ips
	client.id = 
	connections.max.idle.ms = 300000
	default.api.timeout.ms = 60000
	metadata.max.age.ms = 300000
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	receive.buffer.bytes = 65536
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 30000
	retries = 2147483647
	retry.backoff.ms = 100
	sasl.client.callback.handler.class = null
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.login.callback.handler.class = null
	sasl.login.class = null
	sasl.login.refresh.buffer.seconds = 300
	sasl.login.refresh.min.period.seconds = 60
	sasl.login.refresh.window.factor = 0.8
	sasl.login.refresh.window.jitter = 0.05
	sasl.mechanism = GSSAPI
	security.protocol = PLAINTEXT
	security.providers = null
	send.buffer.bytes = 131072
	socket.connection.setup.timeout.max.ms = 30000
	socket.connection.setup.timeout.ms = 10000
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2, TLSv1.3]
	ssl.endpoint.identification.algorithm = https
	ssl.engine.factory.class = null
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.certificate.chain = null
	ssl.keystore.key = null
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLSv1.3
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.certificates = null
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS

24/08/25 20:52:36 WARN AdminClientConfig: The configuration 'key.deserializer' was supplied but isn't a known config.
24/08/25 20:52:36 WARN AdminClientConfig: The configuration 'value.deserializer' was supplied but isn't a known config.
24/08/25 20:52:36 WARN AdminClientConfig: The configuration 'enable.auto.commit' was supplied but isn't a known config.
24/08/25 20:52:36 WARN AdminClientConfig: The configuration 'max.poll.records' was supplied but isn't a known config.
24/08/25 20:52:36 WARN AdminClientConfig: The configuration 'auto.offset.reset' was supplied but isn't a known config.
24/08/25 20:52:36 INFO AppInfoParser: Kafka version: 2.8.1
24/08/25 20:52:36 INFO AppInfoParser: Kafka commitId: 839b886f9b732b15
24/08/25 20:52:36 INFO AppInfoParser: Kafka startTimeMs: 1724619156564
24/08/25 20:52:37 WARN KafkaOffsetReaderAdmin: Error in attempt 1 getting Kafka offsets: 
java.util.concurrent.ExecutionException: org.apache.kafka.common.errors.UnknownTopicOrPartitionException: This server does not host this topic-partition.
	at org.apache.kafka.common.internals.KafkaFutureImpl.wrapAndThrow(KafkaFutureImpl.java:45)
	at org.apache.kafka.common.internals.KafkaFutureImpl.access$000(KafkaFutureImpl.java:32)
	at org.apache.kafka.common.internals.KafkaFutureImpl$SingleWaiter.await(KafkaFutureImpl.java:89)
	at org.apache.kafka.common.internals.KafkaFutureImpl.get(KafkaFutureImpl.java:260)
	at org.apache.spark.sql.kafka010.ConsumerStrategy.retrieveAllPartitions(ConsumerStrategy.scala:66)
	at org.apache.spark.sql.kafka010.ConsumerStrategy.retrieveAllPartitions$(ConsumerStrategy.scala:65)
	at org.apache.spark.sql.kafka010.SubscribeStrategy.retrieveAllPartitions(ConsumerStrategy.scala:102)
	at org.apache.spark.sql.kafka010.SubscribeStrategy.assignedTopicPartitions(ConsumerStrategy.scala:113)
	at org.apache.spark.sql.kafka010.KafkaOffsetReaderAdmin.$anonfun$partitionsAssignedToAdmin$1(KafkaOffsetReaderAdmin.scala:499)
	at org.apache.spark.sql.kafka010.KafkaOffsetReaderAdmin.withRetries(KafkaOffsetReaderAdmin.scala:518)
	at org.apache.spark.sql.kafka010.KafkaOffsetReaderAdmin.partitionsAssignedToAdmin(KafkaOffsetReaderAdmin.scala:498)
	at org.apache.spark.sql.kafka010.KafkaOffsetReaderAdmin.fetchLatestOffsets(KafkaOffsetReaderAdmin.scala:297)
	at org.apache.spark.sql.kafka010.KafkaMicroBatchStream.$anonfun$getOrCreateInitialPartitionOffsets$1(KafkaMicroBatchStream.scala:250)
	at scala.Option.getOrElse(Option.scala:189)
	at org.apache.spark.sql.kafka010.KafkaMicroBatchStream.getOrCreateInitialPartitionOffsets(KafkaMicroBatchStream.scala:245)
	at org.apache.spark.sql.kafka010.KafkaMicroBatchStream.initialOffset(KafkaMicroBatchStream.scala:100)
	at org.apache.spark.sql.execution.streaming.MicroBatchExecution.$anonfun$getStartOffset$2(MicroBatchExecution.scala:457)
	at scala.Option.getOrElse(Option.scala:189)
	at org.apache.spark.sql.execution.streaming.MicroBatchExecution.getStartOffset(MicroBatchExecution.scala:457)
	at org.apache.spark.sql.execution.streaming.MicroBatchExecution.$anonfun$constructNextBatch$4(MicroBatchExecution.scala:491)
	at org.apache.spark.sql.execution.streaming.ProgressReporter.reportTimeTaken(ProgressReporter.scala:427)
	at org.apache.spark.sql.execution.streaming.ProgressReporter.reportTimeTaken$(ProgressReporter.scala:425)
	at org.apache.spark.sql.execution.streaming.StreamExecution.reportTimeTaken(StreamExecution.scala:67)
	at org.apache.spark.sql.execution.streaming.MicroBatchExecution.$anonfun$constructNextBatch$2(MicroBatchExecution.scala:490)
	at scala.collection.TraversableLike.$anonfun$map$1(TraversableLike.scala:286)
	at scala.collection.Iterator.foreach(Iterator.scala:943)
	at scala.collection.Iterator.foreach$(Iterator.scala:943)
	at scala.collection.AbstractIterator.foreach(Iterator.scala:1431)
	at scala.collection.IterableLike.foreach(IterableLike.scala:74)
	at scala.collection.IterableLike.foreach$(IterableLike.scala:73)
	at scala.collection.AbstractIterable.foreach(Iterable.scala:56)
	at scala.collection.TraversableLike.map(TraversableLike.scala:286)
	at scala.collection.TraversableLike.map$(TraversableLike.scala:279)
	at scala.collection.AbstractTraversable.map(Traversable.scala:108)
	at org.apache.spark.sql.execution.streaming.MicroBatchExecution.$anonfun$constructNextBatch$1(MicroBatchExecution.scala:479)
	at scala.runtime.java8.JFunction0$mcZ$sp.apply(JFunction0$mcZ$sp.java:23)
	at org.apache.spark.sql.execution.streaming.MicroBatchExecution.withProgressLocked(MicroBatchExecution.scala:810)
	at org.apache.spark.sql.execution.streaming.MicroBatchExecution.constructNextBatch(MicroBatchExecution.scala:475)
	at org.apache.spark.sql.execution.streaming.MicroBatchExecution.$anonfun$runActivatedStream$2(MicroBatchExecution.scala:268)
	at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)
	at org.apache.spark.sql.execution.streaming.ProgressReporter.reportTimeTaken(ProgressReporter.scala:427)
	at org.apache.spark.sql.execution.streaming.ProgressReporter.reportTimeTaken$(ProgressReporter.scala:425)
	at org.apache.spark.sql.execution.streaming.StreamExecution.reportTimeTaken(StreamExecution.scala:67)
	at org.apache.spark.sql.execution.streaming.MicroBatchExecution.$anonfun$runActivatedStream$1(MicroBatchExecution.scala:249)
	at org.apache.spark.sql.execution.streaming.ProcessingTimeExecutor.execute(TriggerExecutor.scala:67)
	at org.apache.spark.sql.execution.streaming.MicroBatchExecution.runActivatedStream(MicroBatchExecution.scala:239)
	at org.apache.spark.sql.execution.streaming.StreamExecution.$anonfun$runStream$1(StreamExecution.scala:311)
	at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)
	at org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:900)
	at org.apache.spark.sql.execution.streaming.StreamExecution.org$apache$spark$sql$execution$streaming$StreamExecution$$runStream(StreamExecution.scala:289)
	at org.apache.spark.sql.execution.streaming.StreamExecution$$anon$1.$anonfun$run$1(StreamExecution.scala:211)
	at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)
	at org.apache.spark.JobArtifactSet$.withActiveJobArtifactState(JobArtifactSet.scala:94)
	at org.apache.spark.sql.execution.streaming.StreamExecution$$anon$1.run(StreamExecution.scala:211)
Caused by: org.apache.kafka.common.errors.UnknownTopicOrPartitionException: This server does not host this topic-partition.
24/08/25 20:52:38 INFO AppInfoParser: App info kafka.admin.client for adminclient-1 unregistered
24/08/25 20:52:39 INFO Metrics: Metrics scheduler closed
24/08/25 20:52:39 INFO Metrics: Closing reporter org.apache.kafka.common.metrics.JmxReporter
24/08/25 20:52:39 INFO Metrics: Metrics reporters closed
24/08/25 20:52:39 INFO AdminClientConfig: AdminClientConfig values: 
	bootstrap.servers = [kafka:9092]
	client.dns.lookup = use_all_dns_ips
	client.id = 
	connections.max.idle.ms = 300000
	default.api.timeout.ms = 60000
	metadata.max.age.ms = 300000
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	receive.buffer.bytes = 65536
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 30000
	retries = 2147483647
	retry.backoff.ms = 100
	sasl.client.callback.handler.class = null
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.login.callback.handler.class = null
	sasl.login.class = null
	sasl.login.refresh.buffer.seconds = 300
	sasl.login.refresh.min.period.seconds = 60
	sasl.login.refresh.window.factor = 0.8
	sasl.login.refresh.window.jitter = 0.05
	sasl.mechanism = GSSAPI
	security.protocol = PLAINTEXT
	security.providers = null
	send.buffer.bytes = 131072
	socket.connection.setup.timeout.max.ms = 30000
	socket.connection.setup.timeout.ms = 10000
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2, TLSv1.3]
	ssl.endpoint.identification.algorithm = https
	ssl.engine.factory.class = null
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.certificate.chain = null
	ssl.keystore.key = null
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLSv1.3
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.certificates = null
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS

24/08/25 20:52:39 WARN AdminClientConfig: The configuration 'key.deserializer' was supplied but isn't a known config.
24/08/25 20:52:39 WARN AdminClientConfig: The configuration 'value.deserializer' was supplied but isn't a known config.
24/08/25 20:52:39 WARN AdminClientConfig: The configuration 'enable.auto.commit' was supplied but isn't a known config.
24/08/25 20:52:39 WARN AdminClientConfig: The configuration 'max.poll.records' was supplied but isn't a known config.
24/08/25 20:52:39 WARN AdminClientConfig: The configuration 'auto.offset.reset' was supplied but isn't a known config.
24/08/25 20:52:39 INFO AppInfoParser: Kafka version: 2.8.1
24/08/25 20:52:39 INFO AppInfoParser: Kafka commitId: 839b886f9b732b15
24/08/25 20:52:39 INFO AppInfoParser: Kafka startTimeMs: 1724619159056
24/08/25 20:52:39 WARN KafkaOffsetReaderAdmin: Error in attempt 2 getting Kafka offsets: 
java.util.concurrent.ExecutionException: org.apache.kafka.common.errors.UnknownTopicOrPartitionException: This server does not host this topic-partition.
	at org.apache.kafka.common.internals.KafkaFutureImpl.wrapAndThrow(KafkaFutureImpl.java:45)
	at org.apache.kafka.common.internals.KafkaFutureImpl.access$000(KafkaFutureImpl.java:32)
	at org.apache.kafka.common.internals.KafkaFutureImpl$SingleWaiter.await(KafkaFutureImpl.java:89)
	at org.apache.kafka.common.internals.KafkaFutureImpl.get(KafkaFutureImpl.java:260)
	at org.apache.spark.sql.kafka010.ConsumerStrategy.retrieveAllPartitions(ConsumerStrategy.scala:66)
	at org.apache.spark.sql.kafka010.ConsumerStrategy.retrieveAllPartitions$(ConsumerStrategy.scala:65)
	at org.apache.spark.sql.kafka010.SubscribeStrategy.retrieveAllPartitions(ConsumerStrategy.scala:102)
	at org.apache.spark.sql.kafka010.SubscribeStrategy.assignedTopicPartitions(ConsumerStrategy.scala:113)
	at org.apache.spark.sql.kafka010.KafkaOffsetReaderAdmin.$anonfun$partitionsAssignedToAdmin$1(KafkaOffsetReaderAdmin.scala:499)
	at org.apache.spark.sql.kafka010.KafkaOffsetReaderAdmin.withRetries(KafkaOffsetReaderAdmin.scala:518)
	at org.apache.spark.sql.kafka010.KafkaOffsetReaderAdmin.partitionsAssignedToAdmin(KafkaOffsetReaderAdmin.scala:498)
	at org.apache.spark.sql.kafka010.KafkaOffsetReaderAdmin.fetchLatestOffsets(KafkaOffsetReaderAdmin.scala:297)
	at org.apache.spark.sql.kafka010.KafkaMicroBatchStream.$anonfun$getOrCreateInitialPartitionOffsets$1(KafkaMicroBatchStream.scala:250)
	at scala.Option.getOrElse(Option.scala:189)
	at org.apache.spark.sql.kafka010.KafkaMicroBatchStream.getOrCreateInitialPartitionOffsets(KafkaMicroBatchStream.scala:245)
	at org.apache.spark.sql.kafka010.KafkaMicroBatchStream.initialOffset(KafkaMicroBatchStream.scala:100)
	at org.apache.spark.sql.execution.streaming.MicroBatchExecution.$anonfun$getStartOffset$2(MicroBatchExecution.scala:457)
	at scala.Option.getOrElse(Option.scala:189)
	at org.apache.spark.sql.execution.streaming.MicroBatchExecution.getStartOffset(MicroBatchExecution.scala:457)
	at org.apache.spark.sql.execution.streaming.MicroBatchExecution.$anonfun$constructNextBatch$4(MicroBatchExecution.scala:491)
	at org.apache.spark.sql.execution.streaming.ProgressReporter.reportTimeTaken(ProgressReporter.scala:427)
	at org.apache.spark.sql.execution.streaming.ProgressReporter.reportTimeTaken$(ProgressReporter.scala:425)
	at org.apache.spark.sql.execution.streaming.StreamExecution.reportTimeTaken(StreamExecution.scala:67)
	at org.apache.spark.sql.execution.streaming.MicroBatchExecution.$anonfun$constructNextBatch$2(MicroBatchExecution.scala:490)
	at scala.collection.TraversableLike.$anonfun$map$1(TraversableLike.scala:286)
	at scala.collection.Iterator.foreach(Iterator.scala:943)
	at scala.collection.Iterator.foreach$(Iterator.scala:943)
	at scala.collection.AbstractIterator.foreach(Iterator.scala:1431)
	at scala.collection.IterableLike.foreach(IterableLike.scala:74)
	at scala.collection.IterableLike.foreach$(IterableLike.scala:73)
	at scala.collection.AbstractIterable.foreach(Iterable.scala:56)
	at scala.collection.TraversableLike.map(TraversableLike.scala:286)
	at scala.collection.TraversableLike.map$(TraversableLike.scala:279)
	at scala.collection.AbstractTraversable.map(Traversable.scala:108)
	at org.apache.spark.sql.execution.streaming.MicroBatchExecution.$anonfun$constructNextBatch$1(MicroBatchExecution.scala:479)
	at scala.runtime.java8.JFunction0$mcZ$sp.apply(JFunction0$mcZ$sp.java:23)
	at org.apache.spark.sql.execution.streaming.MicroBatchExecution.withProgressLocked(MicroBatchExecution.scala:810)
	at org.apache.spark.sql.execution.streaming.MicroBatchExecution.constructNextBatch(MicroBatchExecution.scala:475)
	at org.apache.spark.sql.execution.streaming.MicroBatchExecution.$anonfun$runActivatedStream$2(MicroBatchExecution.scala:268)
	at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)
	at org.apache.spark.sql.execution.streaming.ProgressReporter.reportTimeTaken(ProgressReporter.scala:427)
	at org.apache.spark.sql.execution.streaming.ProgressReporter.reportTimeTaken$(ProgressReporter.scala:425)
	at org.apache.spark.sql.execution.streaming.StreamExecution.reportTimeTaken(StreamExecution.scala:67)
	at org.apache.spark.sql.execution.streaming.MicroBatchExecution.$anonfun$runActivatedStream$1(MicroBatchExecution.scala:249)
	at org.apache.spark.sql.execution.streaming.ProcessingTimeExecutor.execute(TriggerExecutor.scala:67)
	at org.apache.spark.sql.execution.streaming.MicroBatchExecution.runActivatedStream(MicroBatchExecution.scala:239)
	at org.apache.spark.sql.execution.streaming.StreamExecution.$anonfun$runStream$1(StreamExecution.scala:311)
	at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)
	at org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:900)
	at org.apache.spark.sql.execution.streaming.StreamExecution.org$apache$spark$sql$execution$streaming$StreamExecution$$runStream(StreamExecution.scala:289)
	at org.apache.spark.sql.execution.streaming.StreamExecution$$anon$1.$anonfun$run$1(StreamExecution.scala:211)
	at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)
	at org.apache.spark.JobArtifactSet$.withActiveJobArtifactState(JobArtifactSet.scala:94)
	at org.apache.spark.sql.execution.streaming.StreamExecution$$anon$1.run(StreamExecution.scala:211)
Caused by: org.apache.kafka.common.errors.UnknownTopicOrPartitionException: This server does not host this topic-partition.
24/08/25 20:52:40 INFO AppInfoParser: App info kafka.admin.client for adminclient-2 unregistered
24/08/25 20:52:40 INFO Metrics: Metrics scheduler closed
24/08/25 20:52:40 INFO Metrics: Closing reporter org.apache.kafka.common.metrics.JmxReporter
24/08/25 20:52:40 INFO Metrics: Metrics reporters closed
24/08/25 20:52:40 INFO AdminClientConfig: AdminClientConfig values: 
	bootstrap.servers = [kafka:9092]
	client.dns.lookup = use_all_dns_ips
	client.id = 
	connections.max.idle.ms = 300000
	default.api.timeout.ms = 60000
	metadata.max.age.ms = 300000
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	receive.buffer.bytes = 65536
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 30000
	retries = 2147483647
	retry.backoff.ms = 100
	sasl.client.callback.handler.class = null
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.login.callback.handler.class = null
	sasl.login.class = null
	sasl.login.refresh.buffer.seconds = 300
	sasl.login.refresh.min.period.seconds = 60
	sasl.login.refresh.window.factor = 0.8
	sasl.login.refresh.window.jitter = 0.05
	sasl.mechanism = GSSAPI
	security.protocol = PLAINTEXT
	security.providers = null
	send.buffer.bytes = 131072
	socket.connection.setup.timeout.max.ms = 30000
	socket.connection.setup.timeout.ms = 10000
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2, TLSv1.3]
	ssl.endpoint.identification.algorithm = https
	ssl.engine.factory.class = null
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.certificate.chain = null
	ssl.keystore.key = null
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLSv1.3
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.certificates = null
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS

24/08/25 20:52:40 WARN AdminClientConfig: The configuration 'key.deserializer' was supplied but isn't a known config.
24/08/25 20:52:40 WARN AdminClientConfig: The configuration 'value.deserializer' was supplied but isn't a known config.
24/08/25 20:52:40 WARN AdminClientConfig: The configuration 'enable.auto.commit' was supplied but isn't a known config.
24/08/25 20:52:40 WARN AdminClientConfig: The configuration 'max.poll.records' was supplied but isn't a known config.
24/08/25 20:52:40 WARN AdminClientConfig: The configuration 'auto.offset.reset' was supplied but isn't a known config.
24/08/25 20:52:40 INFO AppInfoParser: Kafka version: 2.8.1
24/08/25 20:52:40 INFO AppInfoParser: Kafka commitId: 839b886f9b732b15
24/08/25 20:52:40 INFO AppInfoParser: Kafka startTimeMs: 1724619160149
24/08/25 20:52:40 WARN KafkaOffsetReaderAdmin: Error in attempt 3 getting Kafka offsets: 
java.util.concurrent.ExecutionException: org.apache.kafka.common.errors.UnknownTopicOrPartitionException: This server does not host this topic-partition.
	at org.apache.kafka.common.internals.KafkaFutureImpl.wrapAndThrow(KafkaFutureImpl.java:45)
	at org.apache.kafka.common.internals.KafkaFutureImpl.access$000(KafkaFutureImpl.java:32)
	at org.apache.kafka.common.internals.KafkaFutureImpl$SingleWaiter.await(KafkaFutureImpl.java:89)
	at org.apache.kafka.common.internals.KafkaFutureImpl.get(KafkaFutureImpl.java:260)
	at org.apache.spark.sql.kafka010.ConsumerStrategy.retrieveAllPartitions(ConsumerStrategy.scala:66)
	at org.apache.spark.sql.kafka010.ConsumerStrategy.retrieveAllPartitions$(ConsumerStrategy.scala:65)
	at org.apache.spark.sql.kafka010.SubscribeStrategy.retrieveAllPartitions(ConsumerStrategy.scala:102)
	at org.apache.spark.sql.kafka010.SubscribeStrategy.assignedTopicPartitions(ConsumerStrategy.scala:113)
	at org.apache.spark.sql.kafka010.KafkaOffsetReaderAdmin.$anonfun$partitionsAssignedToAdmin$1(KafkaOffsetReaderAdmin.scala:499)
	at org.apache.spark.sql.kafka010.KafkaOffsetReaderAdmin.withRetries(KafkaOffsetReaderAdmin.scala:518)
	at org.apache.spark.sql.kafka010.KafkaOffsetReaderAdmin.partitionsAssignedToAdmin(KafkaOffsetReaderAdmin.scala:498)
	at org.apache.spark.sql.kafka010.KafkaOffsetReaderAdmin.fetchLatestOffsets(KafkaOffsetReaderAdmin.scala:297)
	at org.apache.spark.sql.kafka010.KafkaMicroBatchStream.$anonfun$getOrCreateInitialPartitionOffsets$1(KafkaMicroBatchStream.scala:250)
	at scala.Option.getOrElse(Option.scala:189)
	at org.apache.spark.sql.kafka010.KafkaMicroBatchStream.getOrCreateInitialPartitionOffsets(KafkaMicroBatchStream.scala:245)
	at org.apache.spark.sql.kafka010.KafkaMicroBatchStream.initialOffset(KafkaMicroBatchStream.scala:100)
	at org.apache.spark.sql.execution.streaming.MicroBatchExecution.$anonfun$getStartOffset$2(MicroBatchExecution.scala:457)
	at scala.Option.getOrElse(Option.scala:189)
	at org.apache.spark.sql.execution.streaming.MicroBatchExecution.getStartOffset(MicroBatchExecution.scala:457)
	at org.apache.spark.sql.execution.streaming.MicroBatchExecution.$anonfun$constructNextBatch$4(MicroBatchExecution.scala:491)
	at org.apache.spark.sql.execution.streaming.ProgressReporter.reportTimeTaken(ProgressReporter.scala:427)
	at org.apache.spark.sql.execution.streaming.ProgressReporter.reportTimeTaken$(ProgressReporter.scala:425)
	at org.apache.spark.sql.execution.streaming.StreamExecution.reportTimeTaken(StreamExecution.scala:67)
	at org.apache.spark.sql.execution.streaming.MicroBatchExecution.$anonfun$constructNextBatch$2(MicroBatchExecution.scala:490)
	at scala.collection.TraversableLike.$anonfun$map$1(TraversableLike.scala:286)
	at scala.collection.Iterator.foreach(Iterator.scala:943)
	at scala.collection.Iterator.foreach$(Iterator.scala:943)
	at scala.collection.AbstractIterator.foreach(Iterator.scala:1431)
	at scala.collection.IterableLike.foreach(IterableLike.scala:74)
	at scala.collection.IterableLike.foreach$(IterableLike.scala:73)
	at scala.collection.AbstractIterable.foreach(Iterable.scala:56)
	at scala.collection.TraversableLike.map(TraversableLike.scala:286)
	at scala.collection.TraversableLike.map$(TraversableLike.scala:279)
	at scala.collection.AbstractTraversable.map(Traversable.scala:108)
	at org.apache.spark.sql.execution.streaming.MicroBatchExecution.$anonfun$constructNextBatch$1(MicroBatchExecution.scala:479)
	at scala.runtime.java8.JFunction0$mcZ$sp.apply(JFunction0$mcZ$sp.java:23)
	at org.apache.spark.sql.execution.streaming.MicroBatchExecution.withProgressLocked(MicroBatchExecution.scala:810)
	at org.apache.spark.sql.execution.streaming.MicroBatchExecution.constructNextBatch(MicroBatchExecution.scala:475)
	at org.apache.spark.sql.execution.streaming.MicroBatchExecution.$anonfun$runActivatedStream$2(MicroBatchExecution.scala:268)
	at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)
	at org.apache.spark.sql.execution.streaming.ProgressReporter.reportTimeTaken(ProgressReporter.scala:427)
	at org.apache.spark.sql.execution.streaming.ProgressReporter.reportTimeTaken$(ProgressReporter.scala:425)
	at org.apache.spark.sql.execution.streaming.StreamExecution.reportTimeTaken(StreamExecution.scala:67)
	at org.apache.spark.sql.execution.streaming.MicroBatchExecution.$anonfun$runActivatedStream$1(MicroBatchExecution.scala:249)
	at org.apache.spark.sql.execution.streaming.ProcessingTimeExecutor.execute(TriggerExecutor.scala:67)
	at org.apache.spark.sql.execution.streaming.MicroBatchExecution.runActivatedStream(MicroBatchExecution.scala:239)
	at org.apache.spark.sql.execution.streaming.StreamExecution.$anonfun$runStream$1(StreamExecution.scala:311)
	at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)
	at org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:900)
	at org.apache.spark.sql.execution.streaming.StreamExecution.org$apache$spark$sql$execution$streaming$StreamExecution$$runStream(StreamExecution.scala:289)
	at org.apache.spark.sql.execution.streaming.StreamExecution$$anon$1.$anonfun$run$1(StreamExecution.scala:211)
	at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)
	at org.apache.spark.JobArtifactSet$.withActiveJobArtifactState(JobArtifactSet.scala:94)
	at org.apache.spark.sql.execution.streaming.StreamExecution$$anon$1.run(StreamExecution.scala:211)
Caused by: org.apache.kafka.common.errors.UnknownTopicOrPartitionException: This server does not host this topic-partition.
24/08/25 20:52:41 INFO AppInfoParser: App info kafka.admin.client for adminclient-3 unregistered
24/08/25 20:52:41 INFO Metrics: Metrics scheduler closed
24/08/25 20:52:41 INFO Metrics: Closing reporter org.apache.kafka.common.metrics.JmxReporter
24/08/25 20:52:41 INFO Metrics: Metrics reporters closed
24/08/25 20:52:41 ERROR MicroBatchExecution: Query [id = 92810e3c-5257-45d9-9e8a-10b4006ae532, runId = 40ff6c75-883f-4214-b09a-045119f11a66] terminated with error
java.util.concurrent.ExecutionException: org.apache.kafka.common.errors.UnknownTopicOrPartitionException: This server does not host this topic-partition.
	at org.apache.kafka.common.internals.KafkaFutureImpl.wrapAndThrow(KafkaFutureImpl.java:45)
	at org.apache.kafka.common.internals.KafkaFutureImpl.access$000(KafkaFutureImpl.java:32)
	at org.apache.kafka.common.internals.KafkaFutureImpl$SingleWaiter.await(KafkaFutureImpl.java:89)
	at org.apache.kafka.common.internals.KafkaFutureImpl.get(KafkaFutureImpl.java:260)
	at org.apache.spark.sql.kafka010.ConsumerStrategy.retrieveAllPartitions(ConsumerStrategy.scala:66)
	at org.apache.spark.sql.kafka010.ConsumerStrategy.retrieveAllPartitions$(ConsumerStrategy.scala:65)
	at org.apache.spark.sql.kafka010.SubscribeStrategy.retrieveAllPartitions(ConsumerStrategy.scala:102)
	at org.apache.spark.sql.kafka010.SubscribeStrategy.assignedTopicPartitions(ConsumerStrategy.scala:113)
	at org.apache.spark.sql.kafka010.KafkaOffsetReaderAdmin.$anonfun$partitionsAssignedToAdmin$1(KafkaOffsetReaderAdmin.scala:499)
	at org.apache.spark.sql.kafka010.KafkaOffsetReaderAdmin.withRetries(KafkaOffsetReaderAdmin.scala:518)
	at org.apache.spark.sql.kafka010.KafkaOffsetReaderAdmin.partitionsAssignedToAdmin(KafkaOffsetReaderAdmin.scala:498)
	at org.apache.spark.sql.kafka010.KafkaOffsetReaderAdmin.fetchLatestOffsets(KafkaOffsetReaderAdmin.scala:297)
	at org.apache.spark.sql.kafka010.KafkaMicroBatchStream.$anonfun$getOrCreateInitialPartitionOffsets$1(KafkaMicroBatchStream.scala:250)
	at scala.Option.getOrElse(Option.scala:189)
	at org.apache.spark.sql.kafka010.KafkaMicroBatchStream.getOrCreateInitialPartitionOffsets(KafkaMicroBatchStream.scala:245)
	at org.apache.spark.sql.kafka010.KafkaMicroBatchStream.initialOffset(KafkaMicroBatchStream.scala:100)
	at org.apache.spark.sql.execution.streaming.MicroBatchExecution.$anonfun$getStartOffset$2(MicroBatchExecution.scala:457)
	at scala.Option.getOrElse(Option.scala:189)
	at org.apache.spark.sql.execution.streaming.MicroBatchExecution.getStartOffset(MicroBatchExecution.scala:457)
	at org.apache.spark.sql.execution.streaming.MicroBatchExecution.$anonfun$constructNextBatch$4(MicroBatchExecution.scala:491)
	at org.apache.spark.sql.execution.streaming.ProgressReporter.reportTimeTaken(ProgressReporter.scala:427)
	at org.apache.spark.sql.execution.streaming.ProgressReporter.reportTimeTaken$(ProgressReporter.scala:425)
	at org.apache.spark.sql.execution.streaming.StreamExecution.reportTimeTaken(StreamExecution.scala:67)
	at org.apache.spark.sql.execution.streaming.MicroBatchExecution.$anonfun$constructNextBatch$2(MicroBatchExecution.scala:490)
	at scala.collection.TraversableLike.$anonfun$map$1(TraversableLike.scala:286)
	at scala.collection.Iterator.foreach(Iterator.scala:943)
	at scala.collection.Iterator.foreach$(Iterator.scala:943)
	at scala.collection.AbstractIterator.foreach(Iterator.scala:1431)
	at scala.collection.IterableLike.foreach(IterableLike.scala:74)
	at scala.collection.IterableLike.foreach$(IterableLike.scala:73)
	at scala.collection.AbstractIterable.foreach(Iterable.scala:56)
	at scala.collection.TraversableLike.map(TraversableLike.scala:286)
	at scala.collection.TraversableLike.map$(TraversableLike.scala:279)
	at scala.collection.AbstractTraversable.map(Traversable.scala:108)
	at org.apache.spark.sql.execution.streaming.MicroBatchExecution.$anonfun$constructNextBatch$1(MicroBatchExecution.scala:479)
	at scala.runtime.java8.JFunction0$mcZ$sp.apply(JFunction0$mcZ$sp.java:23)
	at org.apache.spark.sql.execution.streaming.MicroBatchExecution.withProgressLocked(MicroBatchExecution.scala:810)
	at org.apache.spark.sql.execution.streaming.MicroBatchExecution.constructNextBatch(MicroBatchExecution.scala:475)
	at org.apache.spark.sql.execution.streaming.MicroBatchExecution.$anonfun$runActivatedStream$2(MicroBatchExecution.scala:268)
	at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)
	at org.apache.spark.sql.execution.streaming.ProgressReporter.reportTimeTaken(ProgressReporter.scala:427)
	at org.apache.spark.sql.execution.streaming.ProgressReporter.reportTimeTaken$(ProgressReporter.scala:425)
	at org.apache.spark.sql.execution.streaming.StreamExecution.reportTimeTaken(StreamExecution.scala:67)
	at org.apache.spark.sql.execution.streaming.MicroBatchExecution.$anonfun$runActivatedStream$1(MicroBatchExecution.scala:249)
	at org.apache.spark.sql.execution.streaming.ProcessingTimeExecutor.execute(TriggerExecutor.scala:67)
	at org.apache.spark.sql.execution.streaming.MicroBatchExecution.runActivatedStream(MicroBatchExecution.scala:239)
	at org.apache.spark.sql.execution.streaming.StreamExecution.$anonfun$runStream$1(StreamExecution.scala:311)
	at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)
	at org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:900)
	at org.apache.spark.sql.execution.streaming.StreamExecution.org$apache$spark$sql$execution$streaming$StreamExecution$$runStream(StreamExecution.scala:289)
	at org.apache.spark.sql.execution.streaming.StreamExecution$$anon$1.$anonfun$run$1(StreamExecution.scala:211)
	at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)
	at org.apache.spark.JobArtifactSet$.withActiveJobArtifactState(JobArtifactSet.scala:94)
	at org.apache.spark.sql.execution.streaming.StreamExecution$$anon$1.run(StreamExecution.scala:211)
Caused by: org.apache.kafka.common.errors.UnknownTopicOrPartitionException: This server does not host this topic-partition.
24/08/25 20:52:41 INFO MicroBatchExecution: Async log purge executor pool for query [id = 92810e3c-5257-45d9-9e8a-10b4006ae532, runId = 40ff6c75-883f-4214-b09a-045119f11a66] has been shutdown
Traceback (most recent call last):
  File "/includes/ingestion/kafka/load-kafka-stream-transactions.py", line 54, in <module>
    query.awaitTermination()
  File "/opt/spark/python/lib/pyspark.zip/pyspark/sql/streaming/query.py", line 221, in awaitTermination
  File "/opt/spark/python/lib/py4j-0.10.9.7-src.zip/py4j/java_gateway.py", line 1322, in __call__
  File "/opt/spark/python/lib/pyspark.zip/pyspark/errors/exceptions/captured.py", line 185, in deco
pyspark.errors.exceptions.captured.StreamingQueryException: [STREAM_FAILED] Query [id = 92810e3c-5257-45d9-9e8a-10b4006ae532, runId = 40ff6c75-883f-4214-b09a-045119f11a66] terminated with exception: org.apache.kafka.common.errors.UnknownTopicOrPartitionException: This server does not host this topic-partition.
INFO:py4j.clientserver:Closing down clientserver connection
24/08/25 20:52:41 INFO SparkContext: Invoking stop() from shutdown hook
24/08/25 20:52:41 INFO SparkContext: SparkContext is stopping with exitCode 0.
24/08/25 20:52:41 INFO SparkUI: Stopped Spark web UI at http://8ef98fa209cc:4040
24/08/25 20:52:41 INFO MapOutputTrackerMasterEndpoint: MapOutputTrackerMasterEndpoint stopped!
24/08/25 20:52:41 INFO MemoryStore: MemoryStore cleared
24/08/25 20:52:41 INFO BlockManager: BlockManager stopped
24/08/25 20:52:41 INFO BlockManagerMaster: BlockManagerMaster stopped
24/08/25 20:52:41 INFO OutputCommitCoordinator$OutputCommitCoordinatorEndpoint: OutputCommitCoordinator stopped!
24/08/25 20:52:41 INFO SparkContext: Successfully stopped SparkContext
24/08/25 20:52:41 INFO ShutdownHookManager: Shutdown hook called
24/08/25 20:52:41 INFO ShutdownHookManager: Deleting directory /tmp/temporary-d5544bc3-2d70-4e62-a908-6946c745f073
24/08/25 20:52:41 INFO ShutdownHookManager: Deleting directory /tmp/spark-294acb29-17cb-48d0-80fd-62fbdaf4210f
24/08/25 20:52:41 INFO ShutdownHookManager: Deleting directory /tmp/spark-67f7a4e8-5fbd-4e39-bd3b-278abd74a879
24/08/25 20:52:41 INFO ShutdownHookManager: Deleting directory /tmp/spark-67f7a4e8-5fbd-4e39-bd3b-278abd74a879/pyspark-6915e148-bb57-436c-a6f1-9906b7fa0328
